{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz4Y3l7FvWf_"
   },
   "outputs": [],
   "source": [
    "#library for running in collab\n",
    "!pip install transformers peft accelerate datasets bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IancFsagU8XL"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nZ_74_qXebG"
   },
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-rw-1b\" #base model\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQ6FoYVQX49j"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Base Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3rIzakGX904"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model for Low-Bit LoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyrgN1mZZSQ7"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LoRA Adapter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-du1y_rZTM1"
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "peft_config = LoraConfig(\n",
    "    r=8, # rank of adapter matrices\n",
    "    lora_alpha=32, # scaling facto\n",
    "    target_modules=[\"query_key_value\"],  # which layers to inject into (model-specific!)\n",
    "    lora_dropout=0.05,  # regularization\n",
    "    bias=\"none\",  # we don't train biases\n",
    "    task_type=\"CAUSAL_LM\" # for language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject LoRA Adapters into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVLNaiuSZXFF"
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This:\n",
    "\n",
    "Unfreezes LayerNorms (helps with adapter training)\n",
    "\n",
    "Adds forward hooks for gradient tracking\n",
    "\n",
    "Keeps quantized model frozen, and prepares for adapter-only training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NSq3PqnZY7p"
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example['instruction'] + \"\\n\" + example['response'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  #  add this line critical for Causal LM loss\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni2t4uQtMMP0"
   },
   "outputs": [],
   "source": [
    "tokenized_data = dataset['train'].map(tokenize)\n",
    "train_dataset=tokenized_data.select(range(2000))  # Try 1kâ€“2k samples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBFLtnwmZcZ2"
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bORE7xfBMQ_3"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    output_dir=\"./qlora-finetuned-model\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    fp16=True # enable mixed precision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2HNWyNCMVTi"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxKx8FmrMbpX"
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "output_dir = \"./qlora-finetuned-model\"\n",
    "checkpoint = get_last_checkpoint(output_dir)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=checkpoint if checkpoint else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Only the LoRA Adapters (Post-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-K7yRbMf9QQ"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qlora-lora-only\")  #  this saves adapter_config.json + adapter_model.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FLxzJmFfVGK"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model = \"tiiuae/falcon-rw-1b\"  # or whatever you used\n",
    "peft_model_path = \"./qlora-finetuned-model\"  # your output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FLGrBWCfoSn"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-E3ksV2gZA5"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # or \"bfloat16\" if supported\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZWXnenvfqOG"
   },
   "outputs": [],
   "source": [
    "# Load model with LoRA adapter\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, \"qlora-lora-only\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK7-gAag1jej"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "prompt = \"Explain why the sky is blue.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "output = model.generate(input_ids, max_new_tokens=100, do_sample=True)\n",
    "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJMwp1Kt2L4K"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Generated Response ===\\n\")\n",
    "print(decoded.replace(\"\\\\n\", \"\\n\"))\n",
    "print(\"\\n==========================\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNF0ze5HpmyBEGa8q3G9Y3q",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
